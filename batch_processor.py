#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†æ‰¹è™•ç†å™¨
æä¾›åˆ†æ‰¹ä¿å­˜å’Œæ¢å¾©åŠŸèƒ½çš„ç¨ç«‹æ¨¡å¡Š
"""

import json
import os
import logging
from datetime import datetime
from typing import Dict, List, Any

logger = logging.getLogger(__name__)

class BatchProcessor:
    """åˆ†æ‰¹è™•ç†å™¨é¡"""
    
    def __init__(self, batch_size: int = 10, base_dir: str = None):
        """åˆå§‹åŒ–åˆ†æ‰¹è™•ç†å™¨"""
        self.batch_size = batch_size
        self.base_dir = base_dir or f"batch_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.progress_file = os.path.join(self.base_dir, "progress.json")
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        os.makedirs(self.base_dir, exist_ok=True)
        
        # åˆå§‹åŒ–é€²åº¦
        self.progress = self._load_progress()
        self.current_batch = {}
        self.current_batch_num = self._get_next_batch_number()
        
        logger.info(f"ğŸ“¦ åˆ†æ‰¹è™•ç†å™¨åˆå§‹åŒ–: æ‰¹æ¬¡å¤§å°={batch_size}, ç›®éŒ„={self.base_dir}")
    
    def _load_progress(self) -> Dict:
        """è¼‰å…¥é€²åº¦æ–‡ä»¶"""
        if os.path.exists(self.progress_file):
            try:
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    progress = json.load(f)
                logger.info(f"ğŸ“‚ è¼‰å…¥é€²åº¦: å·²å®Œæˆ {len(progress.get('completed_rows', []))} æ¢")
                return progress
            except Exception as e:
                logger.warning(f"âš ï¸ è¼‰å…¥é€²åº¦å¤±æ•—: {e}")
        
        return {
            'completed_rows': [],
            'batch_files': [],
            'start_time': datetime.now().isoformat(),
            'last_update': datetime.now().isoformat()
        }
    
    def _save_progress(self):
        """ä¿å­˜é€²åº¦æ–‡ä»¶"""
        try:
            self.progress['last_update'] = datetime.now().isoformat()
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(self.progress, f, ensure_ascii=False, indent=2)
            logger.debug(f"ğŸ’¾ é€²åº¦å·²ä¿å­˜: {len(self.progress.get('completed_rows', []))} æ¢å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜é€²åº¦å¤±æ•—: {e}")
    
    def _get_next_batch_number(self) -> int:
        """ç²å–ä¸‹ä¸€å€‹æ‰¹æ¬¡ç·¨è™Ÿ"""
        batch_files = self.progress.get('batch_files', [])
        if not batch_files:
            return 1
        
        # å¾ç¾æœ‰æ‰¹æ¬¡æ–‡ä»¶ä¸­æ‰¾åˆ°æœ€å¤§ç·¨è™Ÿ
        max_num = 0
        for filename in batch_files:
            if filename.startswith('batch_') and filename.endswith('.json'):
                try:
                    num_str = filename[6:9]  # batch_XXX.json
                    num = int(num_str)
                    max_num = max(max_num, num)
                except:
                    continue
        
        return max_num + 1
    
    def is_processed(self, row_id: int) -> bool:
        """æª¢æŸ¥è¡Œæ˜¯å¦å·²è™•ç†"""
        return row_id in self.progress.get('completed_rows', [])
    
    def add_result(self, row_id: int, result_data: Dict[str, Any]):
        """æ·»åŠ è™•ç†çµæœåˆ°ç•¶å‰æ‰¹æ¬¡"""
        if self.is_processed(row_id):
            logger.info(f"â­ï¸ ç¬¬ {row_id} è¡Œå·²è™•ç†ï¼Œè·³é")
            return False
        
        # æ·»åŠ åˆ°ç•¶å‰æ‰¹æ¬¡
        self.current_batch[str(row_id)] = result_data
        self.progress['completed_rows'].append(row_id)
        
        logger.debug(f"â• æ·»åŠ åˆ°æ‰¹æ¬¡: ç¬¬ {row_id} è¡Œ")
        
        # æª¢æŸ¥æ˜¯å¦éœ€è¦ä¿å­˜æ‰¹æ¬¡
        if len(self.current_batch) >= self.batch_size:
            self._save_current_batch()
        
        return True
    
    def _save_current_batch(self):
        """ä¿å­˜ç•¶å‰æ‰¹æ¬¡"""
        if not self.current_batch:
            return
        
        try:
            batch_filename = f"batch_{self.current_batch_num:03d}.json"
            batch_filepath = os.path.join(self.base_dir, batch_filename)
            
            # æº–å‚™æ‰¹æ¬¡æ•¸æ“š
            batch_data = {
                'metadata': {
                    'batch_number': self.current_batch_num,
                    'batch_size': len(self.current_batch),
                    'created_time': datetime.now().isoformat()
                },
                'results': self.current_batch
            }
            
            # ä¿å­˜æ‰¹æ¬¡æ–‡ä»¶
            with open(batch_filepath, 'w', encoding='utf-8') as f:
                json.dump(batch_data, f, ensure_ascii=False, indent=2)
            
            # æ›´æ–°é€²åº¦è¨˜éŒ„
            if batch_filename not in self.progress.get('batch_files', []):
                self.progress.setdefault('batch_files', []).append(batch_filename)
            
            # ä¿å­˜é€²åº¦
            self._save_progress()
            
            logger.info(f"ğŸ’¾ æ‰¹æ¬¡ {self.current_batch_num} å·²ä¿å­˜: {batch_filename} ({len(self.current_batch)} æ¢)")
            
            # é‡ç½®ç•¶å‰æ‰¹æ¬¡
            self.current_batch = {}
            self.current_batch_num += 1
            
            return batch_filepath
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ‰¹æ¬¡ {self.current_batch_num} å¤±æ•—: {e}")
            return None
    
    def finalize(self, final_filename: str = None) -> str:
        """å®Œæˆè™•ç†ï¼Œä¿å­˜æœ€å¾Œæ‰¹æ¬¡ä¸¦åˆä½µæ‰€æœ‰çµæœ"""
        # ä¿å­˜æœ€å¾Œä¸€å€‹æ‰¹æ¬¡
        if self.current_batch:
            logger.info(f"ğŸ“¦ ä¿å­˜æœ€å¾Œæ‰¹æ¬¡ {self.current_batch_num} ({len(self.current_batch)} æ¢)")
            self._save_current_batch()
        
        # åˆä½µæ‰€æœ‰æ‰¹æ¬¡
        return self._merge_all_batches(final_filename)
    
    def _merge_all_batches(self, final_filename: str = None) -> str:
        """åˆä½µæ‰€æœ‰æ‰¹æ¬¡åˆ°æœ€çµ‚æ–‡ä»¶"""
        try:
            if final_filename is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                final_filename = f"qa_curation_results_{timestamp}.json"
            
            # æ”¶é›†æ‰€æœ‰æ‰¹æ¬¡æ–‡ä»¶
            batch_files = self.progress.get('batch_files', [])
            batch_files.sort()  # æŒ‰æ–‡ä»¶åæ’åº
            
            # åˆä½µçµæœ
            merged_results = {}
            total_processed = 0
            total_success = 0
            
            for batch_file in batch_files:
                batch_path = os.path.join(self.base_dir, batch_file)
                if not os.path.exists(batch_path):
                    logger.warning(f"âš ï¸ æ‰¹æ¬¡æ–‡ä»¶ä¸å­˜åœ¨: {batch_file}")
                    continue
                
                try:
                    with open(batch_path, 'r', encoding='utf-8') as f:
                        batch_data = json.load(f)
                    
                    # åˆä½µçµæœ
                    batch_results = batch_data.get('results', {})
                    merged_results.update(batch_results)
                    
                    # çµ±è¨ˆä¿¡æ¯
                    total_processed += len(batch_results)
                    total_success += sum(1 for r in batch_results.values() if r.get('status') == 'success')
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ è®€å–æ‰¹æ¬¡æ–‡ä»¶ {batch_file} å¤±æ•—: {e}")
            
            # æº–å‚™æœ€çµ‚æ•¸æ“š
            final_data = {
                'metadata': {
                    'processing_start_time': self.progress.get('start_time', ''),
                    'processing_end_time': datetime.now().isoformat(),
                    'total_processed': total_processed,
                    'total_success': total_success,
                    'total_failed': total_processed - total_success,
                    'batch_processing': True,
                    'batch_count': len(batch_files),
                    'batch_size': self.batch_size,
                    'batch_directory': self.base_dir
                },
                'results': merged_results
            }
            
            # ä¿å­˜æœ€çµ‚æ–‡ä»¶
            with open(final_filename, 'w', encoding='utf-8') as f:
                json.dump(final_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… åˆä½µå®Œæˆ: {final_filename}")
            logger.info(f"ğŸ“Š ç¸½è¨ˆ: {total_processed} æ¢ï¼ŒæˆåŠŸ: {total_success} æ¢ï¼Œä¾†è‡ª {len(batch_files)} å€‹æ‰¹æ¬¡")
            
            return final_filename
            
        except Exception as e:
            logger.error(f"âŒ åˆä½µæ‰¹æ¬¡çµæœå¤±æ•—: {e}")
            return None
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–çµ±è¨ˆä¿¡æ¯"""
        return {
            'batch_size': self.batch_size,
            'batch_directory': self.base_dir,
            'completed_rows': len(self.progress.get('completed_rows', [])),
            'batch_files': len(self.progress.get('batch_files', [])),
            'current_batch_size': len(self.current_batch),
            'next_batch_number': self.current_batch_num,
            'start_time': self.progress.get('start_time', ''),
            'last_update': self.progress.get('last_update', '')
        }
    
    def cleanup_batch_files(self, keep_final: bool = True):
        """æ¸…ç†æ‰¹æ¬¡æ–‡ä»¶"""
        try:
            batch_files = self.progress.get('batch_files', [])
            cleaned_count = 0
            
            for batch_file in batch_files:
                batch_path = os.path.join(self.base_dir, batch_file)
                if os.path.exists(batch_path):
                    os.remove(batch_path)
                    cleaned_count += 1
            
            # æ¸…ç†é€²åº¦æ–‡ä»¶
            if os.path.exists(self.progress_file):
                os.remove(self.progress_file)
            
            # å¦‚æœç›®éŒ„ç‚ºç©ºï¼Œåˆªé™¤ç›®éŒ„
            try:
                if not keep_final or not os.listdir(self.base_dir):
                    os.rmdir(self.base_dir)
                    logger.info(f"ğŸ—‘ï¸ å·²æ¸…ç†æ‰¹æ¬¡ç›®éŒ„: {self.base_dir}")
                else:
                    logger.info(f"ğŸ§¹ å·²æ¸…ç† {cleaned_count} å€‹æ‰¹æ¬¡æ–‡ä»¶ï¼Œä¿ç•™ç›®éŒ„: {self.base_dir}")
            except OSError:
                logger.info(f"ğŸ§¹ å·²æ¸…ç† {cleaned_count} å€‹æ‰¹æ¬¡æ–‡ä»¶")
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†æ‰¹æ¬¡æ–‡ä»¶å¤±æ•—: {e}")


def test_batch_processor():
    """æ¸¬è©¦åˆ†æ‰¹è™•ç†å™¨"""
    print("ğŸ§ª æ¸¬è©¦åˆ†æ‰¹è™•ç†å™¨")
    print("=" * 40)
    
    # å‰µå»ºè™•ç†å™¨
    processor = BatchProcessor(batch_size=3)
    
    # æ¨¡æ“¬æ·»åŠ çµæœ
    test_data = [
        {'row': 1, 'score': 85, 'comment': 'å¾ˆå¥½çš„å›ç­”'},
        {'row': 2, 'score': 92, 'comment': 'å„ªç§€çš„åˆ†æ'},
        {'row': 3, 'score': 78, 'comment': 'ä¸éŒ¯çš„è¦‹è§£'},
        {'row': 4, 'score': 88, 'comment': 'æ·±å…¥çš„æ€è€ƒ'},
        {'row': 5, 'score': 95, 'comment': 'å“è¶Šçš„è¡¨ç¾'},
        {'row': 6, 'score': 82, 'comment': 'è‰¯å¥½çš„ç†è§£'},
        {'row': 7, 'score': 90, 'comment': 'ç²¾å½©çš„å›ç­”'},
    ]
    
    print(f"ğŸ“¦ é–‹å§‹è™•ç† {len(test_data)} æ¢æ•¸æ“š...")
    
    for data in test_data:
        row_id = data['row']
        result_data = {
            'score': data['score'],
            'comment': data['comment'],
            'status': 'success',
            'processed_time': datetime.now().isoformat()
        }
        
        success = processor.add_result(row_id, result_data)
        if success:
            print(f"âœ… è™•ç†ç¬¬ {row_id} è¡Œ: åˆ†æ•¸ {data['score']}")
        else:
            print(f"â­ï¸ è·³éç¬¬ {row_id} è¡Œ: å·²è™•ç†")
    
    # å®Œæˆè™•ç†
    final_file = processor.finalize()
    
    # é¡¯ç¤ºçµ±è¨ˆ
    stats = processor.get_stats()
    print(f"\nğŸ“Š è™•ç†çµ±è¨ˆ:")
    print(f"   - å·²å®Œæˆ: {stats['completed_rows']} æ¢")
    print(f"   - æ‰¹æ¬¡æ•¸: {stats['batch_files']} å€‹")
    print(f"   - æ‰¹æ¬¡å¤§å°: {stats['batch_size']} æ¢/æ‰¹æ¬¡")
    print(f"   - æœ€çµ‚æ–‡ä»¶: {final_file}")
    
    # æ¸…ç†
    processor.cleanup_batch_files()
    
    return True

if __name__ == "__main__":
    test_batch_processor()
