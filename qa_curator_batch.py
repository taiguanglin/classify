#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½›å­¸å•ç­”ç²¾é¸å™¨ - åˆ†æ‰¹è™•ç†ç‰ˆæœ¬
æ”¯æŒæ¯10å€‹æ¢ç›®ä¿å­˜ä¸€æ¬¡ï¼Œé¿å…æ•¸æ“šä¸Ÿå¤±
"""

import sys
import os
import argparse
import logging
from datetime import datetime

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ°Pythonè·¯å¾‘
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from qa_curator import BuddhistQACurator
from batch_processor import BatchProcessor

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('qa_curation_batch.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class BuddhistQACuratorBatch(BuddhistQACurator):
    """ä½›å­¸å•ç­”ç²¾é¸å™¨ - åˆ†æ‰¹è™•ç†ç‰ˆæœ¬"""
    
    def __init__(self, config_file: str = 'config.ini', api_key: str = None, 
                 api_type: str = None, chatmock_url: str = None, batch_size: int = 10):
        """åˆå§‹åŒ–åˆ†æ‰¹è™•ç†ç‰ˆæœ¬çš„ç²¾é¸å™¨"""
        super().__init__(config_file, api_key, api_type, chatmock_url)
        self.batch_size = batch_size
        self.batch_processor = None
        logger.info(f"ğŸ“¦ åˆ†æ‰¹è™•ç†ç‰ˆæœ¬åˆå§‹åŒ–å®Œæˆï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
    
    def process_batch_safe(self, start_row: int = None, end_row: int = None, results_file: str = None):
        """å®‰å…¨çš„åˆ†æ‰¹è™•ç†æ–¹æ³•"""
        try:
            # è¨˜éŒ„é–‹å§‹æ™‚é–“
            overall_start_time = datetime.now()
            logger.info(f"ğŸš€ é–‹å§‹å®‰å…¨åˆ†æ‰¹è™•ç† - æ™‚é–“: {overall_start_time.strftime('%H:%M:%S')}")
            
            # è¼‰å…¥é…ç½®
            if start_row is None:
                start_row = self.config.getint('processing', 'start_row', fallback=2)
            if end_row is None:
                config_end_row = self.config.getint('processing', 'end_row', fallback=0)
                end_row = config_end_row if config_end_row > 0 else None
            
            # è¨­ç½®çµæœæ–‡ä»¶å
            if results_file is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                results_file = f'qa_curation_results_{timestamp}.json'
            
            logger.info(f"ğŸ“ æœ€çµ‚çµæœæ–‡ä»¶: {results_file}")
            logger.info(f"ğŸ“¦ åˆ†æ‰¹å¤§å°: {self.batch_size} æ¢/æ‰¹æ¬¡")
            
            # åˆå§‹åŒ–åˆ†æ‰¹è™•ç†å™¨
            self.batch_processor = BatchProcessor(batch_size=self.batch_size)
            
            # è¼‰å…¥Excelæ•¸æ“š
            logger.info("ğŸ“Š è¼‰å…¥Excelæ•¸æ“š...")
            workbook, worksheet = self.load_excel_data()
            logger.info(f"âœ… Excelæ•¸æ“šè¼‰å…¥å®Œæˆ")
            
            # æª¢æŸ¥è™•ç†æ¨¡å¼
            use_filter_mode = self.config.getboolean('processing', 'use_filter_mode', fallback=False)
            
            if use_filter_mode:
                # éæ¿¾æ¨¡å¼
                logger.info("ğŸ” ä½¿ç”¨éæ¿¾æ¨¡å¼...")
                rows_to_process = self.get_filtered_rows(worksheet)
                
                if not rows_to_process:
                    logger.warning("âš ï¸ éæ¿¾æ¨¡å¼ä¸‹æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„è¡Œ")
                    return results_file
                
                logger.info(f"âœ… éæ¿¾å®Œæˆï¼Œæ‰¾åˆ° {len(rows_to_process)} è¡Œ")
                
                # ç²å–éæ¿¾çµæœçš„è©•åˆ†ç¯„åœ
                filter_start_index = self.config.getint('filter', 'start_index', fallback=0)
                filter_end_index = self.config.getint('filter', 'end_index', fallback=0)
                score_all_filtered = self.config.getboolean('filter', 'score_all_filtered', fallback=False)
                
                if score_all_filtered:
                    logger.info("ğŸ¯ è©•åˆ†æ‰€æœ‰éæ¿¾çµæœ")
                elif filter_end_index > 0:
                    start_idx = max(0, filter_start_index)
                    end_idx = min(len(rows_to_process), filter_end_index + 1)
                    rows_to_process = rows_to_process[start_idx:end_idx]
                    logger.info(f"ğŸ¯ éæ¿¾æ¨¡å¼ï¼šè™•ç†ç¬¬ {start_idx+1} åˆ°ç¬¬ {end_idx} æ¢éæ¿¾çµæœï¼Œå…± {len(rows_to_process)} æ¢")
                else:
                    rows_to_process = rows_to_process[:1]
                    logger.info("ğŸ¯ éæ¿¾æ¨¡å¼ï¼šåªè™•ç†ç¬¬ä¸€æ¢éæ¿¾çµæœ")
            else:
                # å‚³çµ±æ¨¡å¼ï¼ˆæŒ‡å®šè¡Œè™Ÿï¼‰
                logger.info("ğŸ“ ä½¿ç”¨è¡Œè™Ÿæ¨¡å¼...")
                max_row = worksheet.max_row
                if end_row is None or end_row > max_row:
                    end_row = max_row
                
                rows_to_process = list(range(start_row, end_row + 1))
                logger.info(f"ğŸ¯ è¡Œè™Ÿæ¨¡å¼ï¼šè™•ç†ç¬¬ {start_row} åˆ° {end_row} è¡Œï¼Œå…± {len(rows_to_process)} æ¢è¨˜éŒ„")
            
            # é–‹å§‹è™•ç†
            total_count = len(rows_to_process)
            processed_count = 0
            success_count = 0
            failed_count = 0
            skipped_count = 0
            
            logger.info(f"ğŸš€ é–‹å§‹è©•åˆ†è™•ç†ï¼Œç¸½ç›®æ¨™: {total_count} æ¢è¨˜éŒ„")
            
            for i, row in enumerate(rows_to_process):
                try:
                    # æª¢æŸ¥æ˜¯å¦å·²è™•ç†
                    if self.batch_processor.is_processed(row):
                        logger.info(f"â­ï¸ ç¬¬ {row} è¡Œå·²è™•ç†ï¼Œè·³é")
                        skipped_count += 1
                        continue
                    
                    # é¡¯ç¤ºé€²åº¦
                    progress_percent = ((i + 1) / total_count) * 100
                    logger.info(f"ğŸ“ˆ é€²åº¦: {i+1}/{total_count} ({progress_percent:.1f}%)")
                    
                    # æå–å•ç­”å…§å®¹
                    logger.info(f"ğŸ“– æå–ç¬¬ {row} è¡Œå•ç­”å…§å®¹...")
                    question, answer = self.extract_qa_content(worksheet, row)
                    
                    if not question and not answer:
                        logger.info(f"âš ï¸ ç¬¬ {row} è¡Œç„¡å…§å®¹ï¼Œè·³é")
                        skipped_count += 1
                        continue
                    
                    logger.info(f"ğŸ”„ è™•ç†ç¬¬ {row} è¡Œ: {question[:100]}...")
                    
                    # é€²è¡Œç²¾é¸è©•åˆ†
                    logger.info(f"ğŸ¤– é–‹å§‹AIè©•åˆ†...")
                    result = self.evaluate_qa_quality(question, answer)
                    logger.info(f"âœ… AIè©•åˆ†å®Œæˆ")
                    
                    # æº–å‚™çµæœæ•¸æ“š
                    result_data = {
                        'row_number': row,
                        'question': question[:500],  # é™åˆ¶é•·åº¦
                        'answer': answer[:1000],     # é™åˆ¶é•·åº¦
                        'breadth_score': result.get('breadth_score', ''),
                        'depth_score': result.get('depth_score', ''),
                        'uniqueness_score': result.get('uniqueness_score', ''),
                        'overall_score': result.get('overall_score', ''),
                        'breadth_comment': result.get('breadth_comment', ''),
                        'depth_comment': result.get('depth_comment', ''),
                        'uniqueness_comment': result.get('uniqueness_comment', ''),
                        'overall_comment': result.get('overall_comment', ''),
                        'question_summary': result.get('question_summary', ''),
                        'answer_summary': result.get('answer_summary', ''),
                        'status': result.get('status', 'success'),
                        'processed_time': datetime.now().isoformat()
                    }
                    
                    # æ·»åŠ åˆ°åˆ†æ‰¹è™•ç†å™¨
                    success = self.batch_processor.add_result(row, result_data)
                    if success:
                        processed_count += 1
                        if result.get('status') == 'success':
                            success_count += 1
                        else:
                            failed_count += 1
                        
                        logger.info(f"âœ… ç¬¬ {row} è¡Œè™•ç†å®Œæˆ")
                    
                    # APIèª¿ç”¨é–“éš”
                    if i < total_count - 1:
                        logger.info(f"â¸ï¸ ç­‰å¾…1ç§’å¾Œè™•ç†ä¸‹ä¸€æ¢...")
                        import time
                        time.sleep(1)
                
                except Exception as e:
                    logger.error(f"âŒ è™•ç†ç¬¬ {row} è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    failed_count += 1
                    processed_count += 1
                    continue
            
            # å®Œæˆè™•ç†ä¸¦ç”Ÿæˆæœ€çµ‚æ–‡ä»¶
            logger.info(f"ğŸ”„ å®Œæˆè™•ç†ï¼Œç”Ÿæˆæœ€çµ‚æ–‡ä»¶...")
            final_file = self.batch_processor.finalize(results_file)
            
            # è¨ˆç®—ç¸½çµ±è¨ˆ
            total_time = (datetime.now() - overall_start_time).total_seconds()
            
            # é¡¯ç¤ºçµ±è¨ˆ
            stats = self.batch_processor.get_stats()
            logger.info(f"ğŸ‰ å®‰å…¨åˆ†æ‰¹è™•ç†å®Œæˆï¼")
            logger.info(f"ğŸ“Š çµ±è¨ˆçµæœ:")
            logger.info(f"   - ç¸½è¨ˆ: {total_count} æ¢")
            logger.info(f"   - æˆåŠŸ: {success_count} æ¢")
            logger.info(f"   - å¤±æ•—: {failed_count} æ¢")
            logger.info(f"   - è·³é: {skipped_count} æ¢")
            logger.info(f"ğŸ“¦ åˆ†æ‰¹è™•ç†çµ±è¨ˆ:")
            logger.info(f"   - æ‰¹æ¬¡å¤§å°: {stats['batch_size']} æ¢/æ‰¹æ¬¡")
            logger.info(f"   - ç¸½æ‰¹æ¬¡æ•¸: {stats['batch_files']} å€‹")
            logger.info(f"   - æ‰¹æ¬¡ç›®éŒ„: {stats['batch_directory']}")
            logger.info(f"â±ï¸ ç¸½è€—æ™‚: {total_time:.2f}ç§’ ({total_time/60:.1f}åˆ†é˜)")
            if processed_count > 0:
                logger.info(f"ğŸš€ å¹³å‡é€Ÿåº¦: {processed_count/total_time:.2f} æ¢/ç§’")
            
            return final_file
            
        except Exception as e:
            logger.error(f"âŒ å®‰å…¨åˆ†æ‰¹è™•ç†å¤±æ•—: {e}")
            raise
    
    def cleanup_batch_files(self, keep_final: bool = True):
        """æ¸…ç†æ‰¹æ¬¡æ–‡ä»¶"""
        if self.batch_processor:
            self.batch_processor.cleanup_batch_files(keep_final)

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(
        description="ä½›å­¸å•ç­”ç²¾é¸è‡ªå‹•åŒ–ç³»çµ± - åˆ†æ‰¹è™•ç†ç‰ˆæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ä½¿ç”¨ChatMockï¼Œæ¯10æ¢ä¿å­˜ä¸€æ¬¡
  python3 qa_curator_batch.py --api-type chatmock --batch-size 10
  
  # ä½¿ç”¨OpenAI APIï¼Œæ¯5æ¢ä¿å­˜ä¸€æ¬¡
  python3 qa_curator_batch.py --api-key YOUR_API_KEY --api-type openai --batch-size 5
  
  # ä½¿ç”¨é»˜èªè¨­ç½®
  python3 qa_curator_batch.py
        """
    )
    
    parser.add_argument(
        '--api-key',
        type=str,
        help='OpenAI API Key'
    )
    
    parser.add_argument(
        '--api-type',
        type=str,
        choices=['openai', 'chatmock'],
        help='APIé¡å‹é¸æ“‡ï¼šopenai æˆ– chatmock'
    )
    
    parser.add_argument(
        '--chatmock-url',
        type=str,
        help='ChatMockæœå‹™å™¨URL'
    )
    
    parser.add_argument(
        '--config',
        type=str,
        default='config.ini',
        help='é…ç½®æ–‡ä»¶è·¯å¾‘'
    )
    
    parser.add_argument(
        '--batch-size',
        type=int,
        default=10,
        help='åˆ†æ‰¹å¤§å°ï¼ˆæ¯æ‰¹æ¬¡ä¿å­˜çš„æ¢ç›®æ•¸ï¼Œé»˜èª10ï¼‰'
    )
    
    parser.add_argument(
        '--cleanup',
        action='store_true',
        help='è™•ç†å®Œæˆå¾Œæ¸…ç†æ‰¹æ¬¡æ–‡ä»¶'
    )
    
    args = parser.parse_args()
    
    print("ä½›å­¸å•ç­”ç²¾é¸è‡ªå‹•åŒ–ç³»çµ± - åˆ†æ‰¹è™•ç†ç‰ˆæœ¬")
    print("=" * 50)
    print(f"ğŸ“¦ åˆ†æ‰¹å¤§å°: {args.batch_size} æ¢/æ‰¹æ¬¡")
    print(f"ğŸ”§ é…ç½®æ–‡ä»¶: {args.config}")
    
    try:
        # å‰µå»ºç²¾é¸å™¨
        curator = BuddhistQACuratorBatch(
            config_file=args.config,
            api_key=args.api_key,
            api_type=args.api_type,
            chatmock_url=args.chatmock_url,
            batch_size=args.batch_size
        )
        
        # åŸ·è¡Œè™•ç†
        results_file = curator.process_batch_safe()
        
        print(f"\nâœ… è™•ç†å®Œæˆï¼")
        print(f"ğŸ“ çµæœæ–‡ä»¶: {results_file}")
        
        # æ¸…ç†æ‰¹æ¬¡æ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if args.cleanup:
            print(f"ğŸ§¹ æ¸…ç†æ‰¹æ¬¡æ–‡ä»¶...")
            curator.cleanup_batch_files()
            print(f"âœ… æ¸…ç†å®Œæˆ")
        else:
            print(f"ğŸ’¡ æ‰¹æ¬¡æ–‡ä»¶å·²ä¿ç•™ï¼Œå¯ç”¨æ–¼æ¢å¾©æˆ–èª¿è©¦")
            stats = curator.batch_processor.get_stats()
            print(f"ğŸ“ æ‰¹æ¬¡ç›®éŒ„: {stats['batch_directory']}")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ ç”¨æˆ¶ä¸­æ–·è™•ç†")
        print(f"ğŸ’¡ å·²è™•ç†çš„æ•¸æ“šå·²ä¿å­˜åˆ°æ‰¹æ¬¡æ–‡ä»¶ä¸­")
        print(f"ğŸ’¡ å¯ä»¥é‡æ–°é‹è¡Œç¨‹åºç¹¼çºŒè™•ç†")
        
    except Exception as e:
        logger.error(f"ç¨‹åºåŸ·è¡Œå¤±æ•—: {e}")
        print(f"âŒ ç¨‹åºåŸ·è¡Œå¤±æ•—: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
